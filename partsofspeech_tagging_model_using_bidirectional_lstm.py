# -*- coding: utf-8 -*-
"""PartsOfSpeech Tagging Model using Bidirectional LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WbH7VTbkqBG9gaPcPuxW3IyW1eVu86Tu
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, TimeDistributed, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
import numpy as np

sentences = [
    ['I', 'am', 'learning', 'TensorFlow'],
    ['You', 'are', 'writing', 'a', 'tutorial'],
    ['They', 'are', 'reading', 'a', 'book'],
    ['She', 'loves', 'machine', 'learning'],
    ['The', 'cat', 'sits', 'on', 'the', 'mat'],
    ['Birds', 'fly', 'in', 'the', 'sky'],
    ['We', 'are', 'building', 'a', 'model'],
    ['He', 'enjoys', 'playing', 'chess'],
    ['This', 'is', 'an', 'exciting', 'project'],
    ['Data', 'science', 'is', 'fun']
]

pos_tags = [
    ['PRON', 'AUX', 'VERB', 'PROPN'],
    ['PRON', 'AUX', 'VERB', 'DET', 'NOUN'],
    ['PRON', 'AUX', 'VERB', 'DET', 'NOUN'],
    ['PRON', 'VERB', 'NOUN', 'NOUN'],
    ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN'],
    ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN'],
    ['PRON', 'AUX', 'VERB', 'DET', 'NOUN'],
    ['PRON', 'VERB', 'VERB', 'NOUN'],
    ['DET', 'AUX', 'DET', 'ADJ', 'NOUN'],
    ['NOUN', 'NOUN', 'AUX', 'ADJ']
]

# Tokenize the sentences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(sentences)

# Tokenize the POS tags
tag_tokenizer = Tokenizer()
tag_tokenizer.fit_on_texts(pos_tags)
tag_index = tag_tokenizer.word_index
tag_sequences = tag_tokenizer.texts_to_sequences(pos_tags)

tokenizer

word_index

sequences

tag_index

tag_sequences

max_len = max([len(s) for s in sequences])
X_train = pad_sequences(sequences, maxlen=max_len, padding='post')
y_train = pad_sequences(tag_sequences, maxlen=max_len, padding='post')

X_train

y_train

num_tags = len(tag_index) + 1  # Add 1 for the padding tag
y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_tags)

print(f"Vocabulary size: {len(word_index)}")
print(f"Number of POS tags: {num_tags}")

y_train

# Padding the sequences for sentences and tags
max_len = max(len(seq) for seq in sequences)
sequences_padded = pad_sequences(sequences, maxlen=max_len, padding='post')
tag_sequences_padded = pad_sequences(tag_sequences, maxlen=max_len, padding='post')

num_tags = len(tag_index)
tag_sequences_onehot = [to_categorical(seq, num_classes=num_tags + 1) for seq in tag_sequences_padded]

# Define model parameters
vocab_size = len(word_index) + 1   # Add 1 for padding token
embedding_dim = 64
lstm_units = 128

model = Sequential([
    Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    TimeDistributed(Dense(num_tags + 1, activation='softmax'))
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(sequences_padded, np.array(tag_sequences_onehot), batch_size=2, epochs=10, verbose=1)
model.summary()

# Predict POS tags for a new sentence
test_sentence =  ['I', 'am', 'learning', 'TensorFlow']
test_sequence = tokenizer.texts_to_sequences([test_sentence])
test_sequence_padded = pad_sequences(test_sequence, maxlen=max_len, padding='post')

# Get predicted tag indices
predictions = model.predict(test_sequence_padded)
predicted_tags = np.argmax(predictions, axis=-1)

# Convert predicted tag indices back to POS tags
predicted_tags_list = [list(tag_index.keys())
                       [list(tag_index.values()).index(p)] for p in predicted_tags[0] if p != 0]

# Print the predicted tags
print(f"Sentence: {test_sentence}")
print(f"Predicted POS tags: {predicted_tags_list}")

